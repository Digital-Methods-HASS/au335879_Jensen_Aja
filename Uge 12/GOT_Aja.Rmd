---
title: "Sentiment analysis about Game of Thrones "
author: "Aja Møller Jensen"
date: "2025-03-19"
output: 
html_document: default
pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!
```

### Get the GOT pdf:
```{r get-document}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```
Some things to notice:

- How cool to extract text out of a PDF! Do you think it will work with any PDF?
- Each row is a page of the PDF (i.e., this is a vector of strings, one for each page)
- The pdf_text() function only sees text that is "selectable"

Example: Just want to get text from a single page (e.g. Page 9)? 
```{r single-page}
got_p9 <- got_text[9]
got_p9 
```
### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```
### Get the tokens (individual words) in tidy format
Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:
```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```
Let's count the words!
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)

got_wc
```
### Remove stop words:

See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons.

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
?stop_words
view(stop_words)
stop_words

got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```
Now check the counts again: 
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)

got_stop 
```
What if we want to get rid of all the numbers (non-text) in the 'got.pdf'?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))

got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)) & word != "chapter") #via chagpt - vi filter så ordet "chapter" ikke er med.
```
### A word cloud of GOT report words (non-numeric)
```{r wordcloud-prep,eval}
# There are almost 2000 unique words, We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?

got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```
Cuztomizing the wordcloud.
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("orchid4","orange","springgreen4")) +
  theme_minimal()
```

###"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")
1

afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

afinn_pos
```
bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```

```{r nrc}
get_sentiments(lexicon = "nrc")
```
### Sentiment analysis with afinn: 
First, bind words in `got.pdf` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
got_afinn2 <- got_afinn %>% 
  filter(value == 2)
```

```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

### NRC lexicon for sentiment analysis
We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the got non-stopword text with the nrc lexicon: 
```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)
got_exclude

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

Now find some counts: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg
```
```{r nrc-confidence}
neg <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "negative")

neg
```
